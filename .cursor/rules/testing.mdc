---
description: PyTorch Testing Best Practices â€” read this before writing tests for up-to-date guidelines on effective PyTorch/Python test design.
globs: 
alwaysApply: false
---
# ðŸ§ª PyTorch + PyTest Testing Guide (Dense)

**Core Principles (from User Protocol):**
- Test after *every* implementation step.
- Tests must be precise and targeted.
- Run all relevant tests; ensure all pass (no skips/failures).
- Fix failures immediately.
- Do not mark tasks complete without verified, passing tests.
- Follow `testing.mtc` (if available) for detailed design, otherwise adhere to these general PyTorch/Python best practices.

**Goal:** Concise best practices for `pytest` + `torch.testing`. Tests in `tests/`.

## ðŸ”§ PyTest Basics

**Structure & Naming:**
- Files: `test_*.py`
- Functions: `test_*()`
- Directory: `tests/` mirrors `visreps/`
- Fixtures: Reusable setup in `conftest.py` (`@pytest.fixture`). Scope: `function`, `class`, `module`, `session`.
```python
# conftest.py
import pytest, torch
@pytest.fixture(scope="module")
def sample_input(): return torch.randn(4, 3, 224, 224)
@pytest.fixture
def simple_model(): return torch.nn.Linear(10, 2)
```

**Parameterization:** `@pytest.mark.parametrize` for multiple inputs.
```python
@pytest.mark.parametrize(("bs", "ch"), [(1,1), (8,3), (32,3)])
def test_model_inputs(simple_model, bs, ch):
    x = torch.randn(bs, ch*5, 10)
    out = simple_model(x[..., -10:])
    assert out.shape[0] == bs
```

**Markers:** Tag (`@pytest.mark.<name>`) & filter (`-m "gpu and not slow"`). Skip (`@pytest.mark.skipif(...)`).

**CLI:**
- `-v`: Verbose
- `-k "pattern"`: Match name
- `-x`: Fail fast
- `--maxfail=N`: Stop after N failures
- `--cov=src`: Coverage (`pytest-cov`)
- `-n auto`: Parallel (`pytest-xdist`)

---

## ðŸ”¬ PyTorch Testing Specifics

**Tensor Comparison:**
- Floats: `torch.testing.assert_close(t1, t2, rtol=..., atol=...)`
- Integers/Exact: `torch.equal(t1, t2)`

**Device Agnosticism:** Parameterize fixtures for devices. Move model/tensors.
```python
@pytest.fixture(params=["cpu", pytest.param("cuda", marks=pytest.mark.gpu)])
def device(request):
    if request.param == "cuda" and not torch.cuda.is_available(): pytest.skip("No CUDA")
    return torch.device(request.param)

def test_on_device(simple_model, device):
    model = simple_model.to(device)
    x = torch.randn(4, 10, device=device)
    assert model(x).device == device
```

**Model & Component Checks:**
- **Units:** Test `nn.Module`s individually.
- **Forward:** Check output `shape`, `dtype`, value range.
- **Init:** Verify custom `reset_parameters`.
- **Grads:** Check `param.grad is not None` after `loss.backward()`. Ensure intended grads flow.
- **Modes:** Test `model.eval()` vs `model.train()` behavior (Dropout, BN).
- **JIT:** Test `torch.jit.script`/`trace` outputs.
```python
def test_fwd_shape(simple_model, sample_input):
    inp = sample_input[:, 0, 0, :10]
    assert simple_model(inp).shape == (inp.shape[0], 2)

def test_grads(simple_model):
    x = torch.randn(1, 10, requires_grad=True)
    simple_model(x).sum().backward()
    assert simple_model.weight.grad is not None
    assert x.grad is not None # If expected

def test_eval_train(dropout_model):
    x = torch.randn(100, 10)
    dropout_model.eval(); y_eval = dropout_model(x)
    dropout_model.train(); y_train = dropout_model(x)
    assert not torch.allclose(y_eval, y_train)
```

**Data Pipeline (`Dataset`/`DataLoader`):**
- `Dataset.__getitem__`: Check item structure, type, shape, basic values. Mock IO.
- `Dataset.__len__`: Check length.
- `DataLoader`: Check batch shape, type, device (`pin_memory`). Test `collate_fn`. Test `num_workers > 0`.
```python
def test_dataset(my_dataset):
    item = my_dataset[0]
    assert isinstance(item, tuple) and len(item) == 2
    assert item[0].shape == (3, 32, 32) and item[1] < 10

def test_loader(my_dataset):
    loader = torch.utils.data.DataLoader(my_dataset, batch_size=4)
    img, lbl = next(iter(loader))
    assert img.shape == (4, 3, 32, 32) and lbl.shape == (4,)
```

**Reproducibility:** Seed `torch`, `numpy`, `random` for stochastic tests. Use judiciously.
```python
import numpy as np, random
def setup_seed(seed=42):
    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)
    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
# Call in tests/fixtures needing determinism
```

**Checkpointing:** Test `state_dict` save/load. Verify model output consistency.
```python
def test_ckpt(simple_model, tmp_path):
    args = (10, 2); x = torch.randn(4, 10)
    out_b4 = simple_model(x)
    fpath = tmp_path / "m.pth"; torch.save(simple_model.state_dict(), fpath)
    new_m = torch.nn.Linear(*args); new_m.load_state_dict(torch.load(fpath))
    new_m.eval(); out_after = new_m(x)
    assert_close(out_b4, out_after)
```

**Mixed Precision (AMP):** Test under `torch.cuda.amp.autocast()`. Check `GradScaler`. Verify `dtype` is `float16`.
```python
@pytest.mark.gpu
def test_amp(simple_model, device):
    if device.type != 'cuda': pytest.skip("AMP needs CUDA")
    model=simple_model.to(device); x=torch.randn(4,10,device=device)
    scaler = torch.cuda.amp.GradScaler()
    with torch.cuda.amp.autocast():
        out = model(x); loss = out.sum()
    # scaler.scale(loss).backward(); scaler.step(opt); scaler.update()
    assert out.dtype == torch.float16
```

**Integration Tests:** Test component interactions (model+optimizer+data). Simulate train/eval steps. Mark as `slow`/`integration`.

---

## ðŸ“ˆ Benchmarking & Profiling

- **Timing:** `torch.utils.benchmark.Timer` for snippets.
- **Profiling:** `torch.profiler` for CPU/GPU activity (run separately).
```python
from torch.utils.benchmark import Timer
def bench_model(simple_model, device): # Separate script ideal
    model=simple_model.to(device); x=torch.randn(64,10,device=device)
    t = Timer(stmt="model(x)", globals=locals())
    print(f"\nMean: {t.timeit(100).mean * 1e3:.2f} ms")
```

---

## âš ï¸ Errors & Debugging

- **Expected Errors:** `pytest.raises(ExpectedError)`.
- **Debugging:** `print((a - b).abs().max())` on `assert_close` fail. Use `pytest --pdb`.
```python
def test_bad_input(simple_model):
    with pytest.raises(RuntimeError):
        simple_model(torch.randn(4, 9)) # Bad shape
```

---

## ðŸ› ï¸ CI/CD

- **Tools:** `pytest`, `pytest-cov`, `black`, `ruff`/`flake8`, `mypy`. Use `pre-commit`.
- **Runs:** CPU tests always. GPU tests nightly/on dedicated runners.
- **Settings:** `pytest -x -vv` in CI.

---

## âœ¨ Extras

- **Default Dtype:** `torch.set_default_dtype()`. Global effect.
- **Memory Leaks:** Monitor `torch.cuda.memory_allocated()` in fixtures for long tests.
- **Property Tests:** `hypothesis` for auto-generating diverse inputs.

---

## ðŸ”— PyTest Plugins

- `pytest-cov`: Coverage.
- `pytest-xdist`: Parallel (`-n auto`).
- `pytest-mock`: Mocking.
- `pytest-rerunfailures`: Retry flaky tests.
- `pytest-timeout`: Test timeouts.
- `pytest-gpu`: GPU management helpers.