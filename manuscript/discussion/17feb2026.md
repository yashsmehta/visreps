Core framing / story

- Field trend: vision models keep moving toward finer supervision—fine-grained classification, then contrastive/self-supervised regimes where effectively each image is its own “class”—typically with massive datasets.

- Your contrast: those choices about “how to carve the world” (WordNet categories, instance discrimination, etc.) are largely arbitrary. You instead ask a systematic, previously unasked question: what granularity of label structure is optimal for brain/behavior alignment?

- Main surprising result: the optimal granularity can be shockingly coarse—in some settings, you can reach near-optimal (or best) alignment with as few as 2 classes. That’s counterintuitive because “binary classification shouldn’t learn anything useful” is the default expectation.
What the result implies (and what you should be careful claiming)

- Theory implication: this is a clean “direction of field vs. unexpected answer” narrative; it challenges the assumption that increasing task granularity is automatically better for brain-like representations.

- Practical implication (tentative): it might suggest ways to reduce annotation / supervision complexity, but you explicitly noted a key caveat: 2-way training doesn’t necessarily reduce compute/data/time and may require similar training budget; also, the learned reps may be brain-aligned but not broadly useful downstream due to training pathologies/artifacts.
Hypotheses about why  coarse training helps

  Two candidate mechanisms were discussed:

1. Stronger categorical axes that match brain axes: coarse supervision may amplify a small number of dominant categorical distinctions that are also present in cortex (e.g., leading PCs become more “brain-like”).
2. More “universal” dimensions, fewer idiosyncratic ones: coarse training may push representations toward shared, dataset-general factors. Concretely: universality scores might increase as classes get coarser.
Analyses / diagnostics you agreed are high-value next

Goal: determine whether coarse-grain training merely “nudges” 1K representations or qualitatively changes them.

1. Universality analysis (Ray’s method)

- Run universality metrics across models trained at different granularities.

- Test: do universality scores systematically increase with coarser training?

2. Eigen-spectrum + PCA structure

- Compare eigenvalue spectra across granularities (especially the 2-class model).
- Look for whether increased brain alignment is concentrated in the top PCs.

3. RDM visualization / “matrix of RDMs”

- Make RDM heatmaps with images ordered by PC1 rank, or grouped by coarse classes.

- Use this as an interpretability probe for emergent structure differences.

4. Representational distance to the 1K model

- Use RSA/RDM similarity between representations of the 2-class model and the 1K model (layerwise).

- Key question: is the 2-class conv4 just “slightly better version of 1K conv4,” or is it noticeably different beyond the score?

5. Training dynamics (optional / heavier)

- Track RSA (brain alignment) across training checkpoints for the coarse model.

- Explore whether it initially resembles 1K along first k PCs then diverges later, or vice versa.

- Acknowledge this likely requires reruns because checkpoints weren’t saved (memory cost).

6. Auxiliary-loss / curriculum idea (future)

- Train 1K with an auxiliary objective derived from coarse structure, or pretrain coarse then finetune to 1K/64-way.

- You’ve tried coarse→finetune and didn’t see clear benefit yet; auxiliary loss remains open.

Robustness / scope points that strengthen the paper

- The “2-way works” effect seems to appear across multiple scales / modalities:

- THINGS behavioral

- spiking / multi-unit dataset (you referenced strong V1-ish results previously)

- NSD fMRI

- Showing the effect across behavior, single-neuron(-like) data, and fMRI is a strong claim: “coarse granularity yields comparable or better alignment across measurement scales.”

Evaluation / reporting details

- Layer selection matters: without layer selection, 1K’s best may look like conv3; with layer search, report best-layer RSA per model (consistent with current approach).

- You should explicitly argue it’s not just dimensionality reduction:

- If coarse models match or outperform a PCA-reconstructed 1K representation baseline (your “PC reconstruction from the 1000-way model” comparison), that supports a qualitative representational change claim.

Known weirdness / artifacts to document (not hide)

- Extremely coarse training can produce training artifacts sensitive to hyperparameters:

- extreme activation values

- low class selectivity index (many units respond to all classes → distributed, potentially “grandmother cell vs distributed” discussion; here it becomes too distributed to be useful for classification)

- Tension to highlight: brain alignment can increase even when downstream utility/class selectivity looks worse, implying brain-alignment metrics are not equivalent to “good features for tasks.”

The final “paper question” as stated

At what granularity do you need to carve up natural image structure to obtain strong alignment with neural and behavioral representations of vision?

You built a systematic training framework across granularities and found the optimum can be surprisingly coarse, in a way that is not explainable by simple dimensionality reduction and likely reflects a real change in representational geometry.
