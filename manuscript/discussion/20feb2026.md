Manuscript discussion summary (Mick Bonner meeting, Feb 20, 2026)

Core story framing (main claim)
- The paper should challenge the field’s default assumption that finer-grained / harder classification objectives (e.g., 1K ImageNet, instance-level contrastive learning) are the best route to brain-like representations.
- Central question: what representational granularity produces strong brain alignment?
- Current results suggest surprisingly coarse-grained supervision can improve alignment.

Critical interpretation constraint (avoid overclaiming)
- Do not claim the brain is literally optimized for “2 classes” or coarse labels.
- Do not infer from lower model dimensionality that the brain itself is low-dimensional.
- Proper claim: coarse supervision may induce representational principles / geometry that are more brain-like.

⸻

Highest-priority next analyses (narrative-first, not just more results)

Mick prioritized analyses that explain why coarse-graining helps.

1) PCA-based qualitative visualizations (top priority)
- Most/least activating images per PC (PC extremes) to interpret major organizing axes.
- PC1–PC2 scatter plots comparing 1K vs coarse-trained models using a shared category/coloring scheme.
- Goal: test whether coarse training yields clearer categorical organization along key axes (e.g., stronger separation, less ambiguous central mass).
- These are core to a mechanistic “geometry” story, not just score reporting.

2) RDM visualizations (high priority)
- Compare behavioral RDMs vs model RDMs (especially 1K vs coarse).
- Aim: support the intuition that humans may emphasize major categorical distinctions more strongly than standard 1K-trained models, and coarse training may better recover that structure.

⸻

Effective dimensionality vs eigenspectra (important methodological caution)

You observed coarse models have lower effective dimensionality (ED), but Mick cautioned against strong interpretation.

Key point
- ED alone is not reliable enough for interpretation.
- Different eigenspectra can produce similar ED; ED can be distorted by odd variance structure (e.g., unusually dominant first PC / high-variance channels).

Recommended approach
- Use eigenspectra across layers as the primary diagnostic.
- Keep ED as a compact summary statistic, but interpret only alongside eigenspectra.

Interpretation implication
- Even if coarse models are lower-dimensional internally, that does not mean the alignment effect is simply “brain is lower-dimensional.”
- ED differences should be treated as descriptive unless tied to a clearer geometric account.

⸻

Visualization methodology decision (AlexNet mismatch)

You raised a mismatch:
- The coarse-class coloring/visualization setup was derived from a PyTorch pretrained AlexNet.
- The analyzed 1K model is a locally trained AlexNet-like model, so representations differ.

Mick’s decision (acceptable for visualization)
- First plot the network used to define the classes (reference).
- Then plot the actual analyzed networks using the same category/coloring scheme.

Why this is okay:
- These are intuition/interpretation visualizations, not the primary quantitative evidence.
- They can still reveal whether coarse training shifts geometry toward clearer categorical structure.

⸻

Emerging “why” explanation (most promising narrative thread)

A strong interpretation emerged:
- Human brain/behavior may emphasize a small number of key organizing distinctions more strongly than conventional 1K supervision does.
- Coarse supervision may better shape those dominant distinctions.
- This can explain improved RSA as more human-like representational geometry, especially in the unweighted / no-fit setting.

This is the strongest current manuscript-level explanatory story:
- not “coarse is universally better,”
- but “coarse supervision may emphasize the right dimensions more strongly.”

⸻

Encoding analyses (targeted, reviewer-relevant, conceptually important)

Mick recommended running encoding analyses selectively (not exhaustively), mainly to shape the narrative and address likely reviewer concerns.

Why run encoding now
- NeurIPS reviewers previously asked for encoding results.
- Whether encoding matches RSA affects how to frame the contribution.

Suggested scope
- Run a targeted encoding analysis on a key case (e.g., CLIP coarse-grain, where RSA effect is strongest).

Narrative implications
- If encoding matches RSA trend: strengthens the result.
- If encoding differs from RSA: still fine, but must be explicitly framed:
- coarse-graining improves unweighted representational alignment (RSA),
- while 1K models may recover alignment after linear reweighting (encoding),
- implying coarse training yields features that are already weighted in a more brain-like way.

This was treated as a meaningful distinction, not a problem.

⸻

Follow-up neuroscience analyses (secondary to visualization story)

Mick viewed these as worthwhile but not first priority relative to the PCA/RDM visualization package.

Additional NSD ROIs (recommended)
Good candidates because they are easy to run (mostly swapping ROI definitions) and may strengthen the neuroscience story:
- Category-selective ROIs (faces/scenes/objects): may show larger coarse-grain benefits due to strong category coding.
- Early visual ROIs (V1/V2/V3/V4): especially interesting because monkey results showed strong coarse-grain benefits in early areas; test whether similar trends hold in humans.

Whole-brain mapping (not prioritized now)
- More overhead / more cumbersome.
- Lower immediate payoff for the central manuscript narrative.

⸻

Journal framing (audience positioning)
- Mick suggested aiming first at Nature Neuroscience (with Machine Intelligence also viable).
- The core framing should still be broad:
- What drives alignment between neural networks and the human visual system?
- The manuscript should clearly speak to:
- visual/computational neuroscience,
- representation learning / machine learning.

The main priority is to build a strong theoretical + mechanistic story first; journal-specific tailoring can happen later.

⸻

Figure communication / editorial packaging
- Use different visual/color styles for qualitatively different result types (e.g., NSD vs THINGS behavior vs spiking).
- Rationale: helps editors/readers quickly recognize that the paper contains multiple distinct evidence streams, rather than repeated versions of one result.

⸻

Net takeaway (compressed)

The meeting sharpened the manuscript strategy around a mechanistic, geometry-based explanation of why coarse-grained supervision improves brain alignment. The immediate priority is a visualization-first package (PC extremes, PC scatter geometry, RDMs, eigenspectra) that supports a “coarse supervision emphasizes the right organizing dimensions” story, while avoiding overclaims about literal brain objectives or dimensionality. Targeted encoding analyses (especially a strong CLIP case) are important for narrative robustness and reviewer-proofing, and additional NSD ROI tests are worthwhile but secondary to establishing the core explanatory figures.
